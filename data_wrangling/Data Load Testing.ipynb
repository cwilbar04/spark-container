{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa4a07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env GOOGLE_APPLICATION_CREDENTIALS=/home/pitfox/data/spark-container-dev-f5d53ab2439c.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $GOOGLE_APPLICATION_CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $SPARK_HOME/conf/spark-defaults.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d84d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, ArrayType, LongType\n",
    "import pyspark.sql.functions as f\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74012450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "078ee7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Load Review Files\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/spark-bigquery-with-dependencies_2.12-0.20.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21d42a59-7cee-4be3-97e8-97a149107f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to your GCS bucket\n",
    "gcs_bucket = f'amazon_reviews_bucket'\n",
    "gcs_filepath = f'gs://amazon_reviews_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10e2ac26-1520-49d8-9f3c-05186c85cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema of files to parse\n",
    "schema = StructType([ \n",
    "    StructField(\"asin\",StringType(),True), \n",
    "    StructField(\"image\",ArrayType(StringType()),True), \n",
    "    StructField(\"overall\",DoubleType(),True),\n",
    "    StructField(\"reviewText\",StringType(),True),\n",
    "    StructField(\"reviewTime\",StringType(),True),\n",
    "    StructField(\"reviewerID\",StringType(),True),\n",
    "    StructField(\"reviewerName\",StringType(),True),\n",
    "    StructField(\"summary\",StringType(),True),\n",
    "    StructField(\"unixReviewTime\",LongType(),True),\n",
    "    StructField(\"verified\",BooleanType(),True),\n",
    "    StructField(\"vote\",StringType(),True),\n",
    "    StructField(\"style\",ArrayType(StringType()),True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ecde0d4-9b16-4ab2-ba71-f3deb681f737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/AMAZON_FASHION_5.json.gz\n",
      "AMAZON_FASHION_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/All_Beauty_5.json.gz\n",
      "All_Beauty_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Appliances_5.json.gz\n",
      "Appliances_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Arts_Crafts_and_Sewing_5.json.gz\n",
      "Arts_Crafts_and_Sewing_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Automotive_5.json.gz\n",
      "Automotive_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Books_5.json.gz\n",
      "Books_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/CDs_and_Vinyl_5.json.gz\n",
      "CDs_and_Vinyl_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Cell_Phones_and_Accessories_5.json.gz\n",
      "Cell_Phones_and_Accessories_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Clothing_Shoes_and_Jewelry_5.json.gz\n",
      "Clothing_Shoes_and_Jewelry_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Digital_Music_5.json.gz\n",
      "Digital_Music_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Electronics_5.json.gz\n",
      "Electronics_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Gift_Cards_5.json.gz\n",
      "Gift_Cards_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Grocery_and_Gourmet_Food_5.json.gz\n",
      "Grocery_and_Gourmet_Food_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Home_and_Kitchen_5.json.gz\n",
      "Home_and_Kitchen_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Industrial_and_Scientific_5.json.gz\n",
      "Industrial_and_Scientific_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Kindle_Store_5.json.gz\n",
      "Kindle_Store_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Luxury_Beauty_5.json.gz\n",
      "Luxury_Beauty_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Magazine_Subscriptions_5.json.gz\n",
      "Magazine_Subscriptions_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz\n",
      "Movies_and_TV_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Musical_Instruments_5.json.gz\n",
      "Musical_Instruments_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Office_Products_5.json.gz\n",
      "Office_Products_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Patio_Lawn_and_Garden_5.json.gz\n",
      "Patio_Lawn_and_Garden_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Pet_Supplies_5.json.gz\n",
      "Pet_Supplies_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Prime_Pantry_5.json.gz\n",
      "Prime_Pantry_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Software_5.json.gz\n",
      "Software_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Sports_and_Outdoors_5.json.gz\n",
      "Sports_and_Outdoors_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Tools_and_Home_Improvement_5.json.gz\n",
      "Tools_and_Home_Improvement_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Toys_and_Games_5.json.gz\n",
      "Toys_and_Games_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\n",
      "Video_Games_5.json.gz\n"
     ]
    }
   ],
   "source": [
    "# URL to scrape to get files to download\n",
    "url = \"https://nijianmo.github.io/amazon/index.html\"\n",
    "html = requests.get(url)\n",
    "\n",
    "if html.ok:\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')  \n",
    "\n",
    "output_final = []\n",
    "files = []\n",
    "links = soup.find_all('a',string='5-core')#.find('5-core')#.find_all('td', id='5-core')\n",
    "for link in links:\n",
    "    url = link.get('href')\n",
    "    file = url.split('/')[-1]\n",
    "    print(url)\n",
    "    print(url.split('/')[-1])\n",
    "    spark.sparkContext.addFile(url)\n",
    "    files.append(file)\n",
    "    df = spark.read.json(\"file://\"+SparkFiles.get(file),schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a0fcf8-cd5b-4491-898e-e6f39d5a428e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/AMAZON_FASHION_5.json.gz\n",
      "AMAZON_FASHION_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/All_Beauty_5.json.gz\n",
      "All_Beauty_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Appliances_5.json.gz\n",
      "Appliances_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Arts_Crafts_and_Sewing_5.json.gz\n",
      "Arts_Crafts_and_Sewing_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Automotive_5.json.gz\n",
      "Automotive_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Books_5.json.gz\n",
      "Books_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/CDs_and_Vinyl_5.json.gz\n",
      "CDs_and_Vinyl_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Cell_Phones_and_Accessories_5.json.gz\n",
      "Cell_Phones_and_Accessories_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Clothing_Shoes_and_Jewelry_5.json.gz\n",
      "Clothing_Shoes_and_Jewelry_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Digital_Music_5.json.gz\n",
      "Digital_Music_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Electronics_5.json.gz\n",
      "Electronics_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Gift_Cards_5.json.gz\n",
      "Gift_Cards_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Grocery_and_Gourmet_Food_5.json.gz\n",
      "Grocery_and_Gourmet_Food_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Home_and_Kitchen_5.json.gz\n",
      "Home_and_Kitchen_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Industrial_and_Scientific_5.json.gz\n",
      "Industrial_and_Scientific_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Kindle_Store_5.json.gz\n",
      "Kindle_Store_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Luxury_Beauty_5.json.gz\n",
      "Luxury_Beauty_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Magazine_Subscriptions_5.json.gz\n",
      "Magazine_Subscriptions_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz\n",
      "Movies_and_TV_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Musical_Instruments_5.json.gz\n",
      "Musical_Instruments_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Office_Products_5.json.gz\n",
      "Office_Products_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Patio_Lawn_and_Garden_5.json.gz\n",
      "Patio_Lawn_and_Garden_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Pet_Supplies_5.json.gz\n",
      "Pet_Supplies_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Prime_Pantry_5.json.gz\n",
      "Prime_Pantry_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Software_5.json.gz\n",
      "Software_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Sports_and_Outdoors_5.json.gz\n",
      "Sports_and_Outdoors_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Tools_and_Home_Improvement_5.json.gz\n",
      "Tools_and_Home_Improvement_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Toys_and_Games_5.json.gz\n",
      "Toys_and_Games_5.json.gz\n",
      "http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Video_Games_5.json.gz\n",
      "Video_Games_5.json.gz\n",
      "loaded first file: AMAZON_FASHION_5.json.gz\n",
      "appended to df with: All_Beauty_5.json.gz\n",
      "appended to df with: Appliances_5.json.gz\n",
      "appended to df with: Arts_Crafts_and_Sewing_5.json.gz\n",
      "appended to df with: Automotive_5.json.gz\n",
      "appended to df with: Books_5.json.gz\n",
      "appended to df with: CDs_and_Vinyl_5.json.gz\n",
      "appended to df with: Cell_Phones_and_Accessories_5.json.gz\n",
      "appended to df with: Clothing_Shoes_and_Jewelry_5.json.gz\n",
      "appended to df with: Digital_Music_5.json.gz\n",
      "appended to df with: Electronics_5.json.gz\n",
      "appended to df with: Gift_Cards_5.json.gz\n",
      "appended to df with: Grocery_and_Gourmet_Food_5.json.gz\n",
      "appended to df with: Home_and_Kitchen_5.json.gz\n",
      "appended to df with: Industrial_and_Scientific_5.json.gz\n",
      "appended to df with: Kindle_Store_5.json.gz\n",
      "appended to df with: Luxury_Beauty_5.json.gz\n",
      "appended to df with: Magazine_Subscriptions_5.json.gz\n",
      "appended to df with: Movies_and_TV_5.json.gz\n",
      "appended to df with: Musical_Instruments_5.json.gz\n",
      "appended to df with: Office_Products_5.json.gz\n",
      "appended to df with: Patio_Lawn_and_Garden_5.json.gz\n",
      "appended to df with: Pet_Supplies_5.json.gz\n",
      "appended to df with: Prime_Pantry_5.json.gz\n",
      "appended to df with: Software_5.json.gz\n",
      "appended to df with: Sports_and_Outdoors_5.json.gz\n",
      "appended to df with: Tools_and_Home_Improvement_5.json.gz\n",
      "appended to df with: Toys_and_Games_5.json.gz\n",
      "appended to df with: Video_Games_5.json.gz\n"
     ]
    }
   ],
   "source": [
    "# URL to scrape to get files to download\n",
    "url = \"https://nijianmo.github.io/amazon/index.html\"\n",
    "html = requests.get(url)\n",
    "\n",
    "if html.ok:\n",
    "    soup = BeautifulSoup(html.content, 'html.parser')  \n",
    "\n",
    "output_final = []\n",
    "files = []\n",
    "links = soup.find_all('a',string='5-core')#.find('5-core')#.find_all('td', id='5-core')\n",
    "for link in links:\n",
    "    url = link.get('href')\n",
    "    file = url.split('/')[-1]\n",
    "    print(url)\n",
    "    print(url.split('/')[-1])\n",
    "    spark.sparkContext.addFile(url)\n",
    "    files.append(file)\n",
    "\n",
    "for file in files:    \n",
    "    df = spark.read.json(\"file://\"+SparkFiles.get(file),schema)\n",
    "    df = df.dropDuplicates() \n",
    "    df = df.withColumn('review_wordCount', f.size(f.split(f.col('reviewText'), ' ')))\n",
    "    df.registerTempTable(\"dataframe\")\n",
    "    sql_script = f\"\"\"select \n",
    "              '{file}' as category,\n",
    "              asin || '-' || reviewerID || row_number() OVER (PARTITION BY asin, reviewerID ORDER BY unixReviewTime asc) as review_ID,\n",
    "              asin as product_ID,\n",
    "              reviewerID as reviewer_ID,\n",
    "              overall as rating_out_of_5,\n",
    "              summary as review_summary,\n",
    "              reviewText as review_text,\n",
    "              review_wordCount as review_word_count,     \n",
    "              '{url}' as source_url\n",
    "            from dataframe\"\"\"\n",
    "    output = spark.sql(sql_script)\n",
    "    if not output_final:\n",
    "        output_final = output\n",
    "        print(f'loaded first file: {file}')\n",
    "    else:\n",
    "        output_final = output_final.union(output)\n",
    "        print(f'appended to df with: {file}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d811149-950b-4a43-9b86-d4f3a717c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0dbd9bd-8519-4523-9b53-b59eccd2033b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o558.save.\n: java.lang.RuntimeException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:94)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:43)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:112)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:89)\n\t... 34 more\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 4425) (0a588482698e executor driver): java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:347)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 56 more\nCaused by: java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:347)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-70b5ee345dda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigquery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temporaryGcsBucket\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgcs_bucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"amazon_reviews.categoryFilesSmall\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o558.save.\n: java.lang.RuntimeException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:94)\n\tat com.google.cloud.spark.bigquery.BigQueryInsertableRelation.insert(BigQueryInsertableRelation.scala:43)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:112)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat com.google.cloud.spark.bigquery.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.scala:89)\n\t... 34 more\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 4425) (0a588482698e executor driver): java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:347)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 56 more\nCaused by: java.io.IOException: No space left on device\n\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:347)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:59)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:223)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:176)\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81)\n\tat java.base/java.io.BufferedOutputStream.write(BufferedOutputStream.java:127)\n\tat java.base/java.io.DataOutputStream.write(DataOutputStream.java:106)\n\tat org.apache.spark.sql.catalyst.expressions.UnsafeRow.writeToStream(UnsafeRow.java:539)\n\tat org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:69)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "output_final.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"temporaryGcsBucket\",gcs_bucket) \\\n",
    "  .mode(\"append\") \\\n",
    "  .save(\"amazon_reviews.categoryFilesSmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de3fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sparkContext._jvm.scala.util.Properties.versionString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0bcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['AMAZON_FASHION','All_Beauty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aacc6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/{category[1]}_5.json.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff611a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d48f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"file://\"+SparkFiles.get(f\"{category[1]}_5.json.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd97e7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a482818-9422-4b81-b6c0-f5e2402986a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType, BooleanType, ArrayType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2ec5c-5299-4e1b-ada3-8d642ff33040",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    "    StructField(\"asin\",StringType(),True), \n",
    "    StructField(\"image\",ArrayType(StringType()),True), \n",
    "    StructField(\"overall\",DoubleType(),True),\n",
    "    StructField(\"reviewText\",StringType(),True),\n",
    "    StructField(\"reviewTime\",StringType(),True),\n",
    "    StructField(\"reviewerID\",StringType(),True),\n",
    "    StructField(\"reviewerName\",StringType(),True),\n",
    "    StructField(\"summary\",StringType(),True),\n",
    "    StructField(\"unixReviewedTime\",LongType(),True),\n",
    "    StructField(\"verified\",BooleanType(),True),\n",
    "    StructField(\"vote\",StringType(),True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates() \n",
    "df = df.withColumn('review_wordCount', f.size(f.split(f.col('reviewText'), ' ')))\n",
    "df.registerTempTable(\"dataframe\")\n",
    "sql_script = f\"\"\"select \n",
    "          '{category[1]}' as category,\n",
    "          asin || '-' || reviewerID || row_number() OVER (PARTITION BY asin, reviewerID ORDER BY unixReviewTime asc) as review_ID,\n",
    "          asin as product_ID,\n",
    "          reviewerID as reviewer_ID,\n",
    "          overall as rating_out_of_5,\n",
    "          summary as review_summary,\n",
    "          reviewText as review_text,\n",
    "          review_wordCount as review_word_count,     \n",
    "          '{url}' as source_url\n",
    "        from dataframe\"\"\"\n",
    "output = spark.sql(sql_script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32bef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f\"\"\"select \n",
    "          '{category[1]}' as category,\n",
    "          asin || '-' || reviewerID || row_number() OVER (PARTITION BY asin, reviewerID ORDER BY unixReviewTime asc) as review_ID,\n",
    "          asin as product_ID,\n",
    "          reviewerID as reviewer_ID,\n",
    "          overall as rating_out_of_5,\n",
    "          summary as review_summary,\n",
    "          reviewText as review_text,\n",
    "          review_wordCount as review_word_count,     \n",
    "          '{url}' as source_url\n",
    "        from dataframe\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = spark.sql(sql_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to your GCS bucket\n",
    "gcs_bucket = f'amazon_reviews_bucket'\n",
    "\n",
    "gcs_filepath = f'gs://amazon_reviews_bucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb59b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"temporaryGcsBucket\",gcs_bucket) \\\n",
    "  .mode(\"append\") \\\n",
    "  .save(\"amazon_reviews.categoryFilesSmall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.write.partitionBy(\"product_ID\").csv(gcs_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.write \\\n",
    "#   .mode('overwrite') \\\n",
    "#   .csv(gcs_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8905636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eabf8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AMAZON_FASHION_5.json.gz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/AMAZON_FASHION_5.json.gz'\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf0f29b-892a-49eb-a79a-2cbb87f66582",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'AMAZON_FASHION_5.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e5db2d-88d9-4ab5-b96c-aea1295be3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "with gzip.open(file, 'rb') as f_in:\n",
    "    with open('.'.join(file.split('.')[0:2]), 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b135020-09a1-431a-85e1-88a299bd4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf057800-8f13-41a1-9319-33e32330d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'.'.join(file.split('.')[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10eaf0e2-cad9-43e0-b52a-66b11d1bf10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb5cd467-5f9e-48bf-870b-d0f100af80ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"file://\"+SparkFiles.get(file),schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f625840-e196-453e-a0f6-de44ce2d897a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+--------------------+----------+-------------+------------+----------+--------------+--------+----+\n",
      "|      asin|image|overall|          reviewText|reviewTime|   reviewerID|reviewerName|   summary|unixReviewTime|verified|vote|\n",
      "+----------+-----+-------+--------------------+----------+-------------+------------+----------+--------------+--------+----+\n",
      "|B000K2PJ4K| null|    5.0|Great product and...|09 4, 2015|ALJ66O1Y6SLHA|    Tonya B.|Five Stars|    1441324800|    true|null|\n",
      "|B000K2PJ4K| null|    5.0|Great product and...|09 4, 2015|ALJ66O1Y6SLHA|    Tonya B.|Five Stars|    1441324800|    true|null|\n",
      "|B000K2PJ4K| null|    5.0|Great product and...|09 4, 2015|ALJ66O1Y6SLHA|    Tonya B.|Five Stars|    1441324800|    true|null|\n",
      "|B000K2PJ4K| null|    5.0|Great product and...|09 4, 2015|ALJ66O1Y6SLHA|    Tonya B.|Five Stars|    1441324800|    true|null|\n",
      "|B000K2PJ4K| null|    5.0|Great product and...|09 4, 2015|ALJ66O1Y6SLHA|    Tonya B.|Five Stars|    1441324800|    true|null|\n",
      "+----------+-----+-------+--------------------+----------+-------------+------------+----------+--------------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8859aecf-cf1b-46e2-8820-1628b141e00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e194d9f-08ef-43d0-812b-d55801e44b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00752702-5021-4876-8f52-4582e8745153",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"file://\"+SparkFiles.get(file),schema)\n",
    "df = df.dropDuplicates(['reviewerID','overall','summary','reviewText']) \n",
    "df = df.withColumn('review_wordCount', f.size(f.split(f.col('reviewText'), ' ')))\n",
    "df.registerTempTable(\"dataframe\")\n",
    "sql_script = f\"\"\"select \n",
    "          '{file.split('.')[0]}' as category,\n",
    "          asin || '-' || reviewerID || '-' || row_number() OVER (PARTITION BY reviewerID ORDER BY unixReviewTime asc) as review_ID,\n",
    "          asin as product_ID,\n",
    "          reviewerID as reviewer_ID,\n",
    "          overall as rating_out_of_5,\n",
    "          summary as review_summary,\n",
    "          reviewText as review_text,\n",
    "          review_wordCount as review_word_count,  \n",
    "          verified,\n",
    "          vote,\n",
    "          reviewTime,\n",
    "          unixReviewTime,\n",
    "          image,\n",
    "          '{url}' as source_url\n",
    "        from dataframe\"\"\"\n",
    "output = spark.sql(sql_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8746856-0223-407f-8c6c-b45fa084b3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47d40110-33ff-4b38-8b08-2bba776edae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+----------+--------------+---------------+--------------------+--------------------+-----------------+--------+----+-----------+--------------+-----+--------------------+\n",
      "|        category|           review_ID|product_ID|   reviewer_ID|rating_out_of_5|      review_summary|         review_text|review_word_count|verified|vote| reviewTime|unixReviewTime|image|          source_url|\n",
      "+----------------+--------------------+----------+--------------+---------------+--------------------+--------------------+-----------------+--------+----+-----------+--------------+-----+--------------------+\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A29K85...|B001IKJOLW|A29K85C92Z8HGR|            4.0|         Light shoes|It's been a long ...|               86|    true|null| 07 7, 2017|    1499385600| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A2JOUO...|B001IKJOLW|A2JOUOASELY1XR|            5.0|I feel like this ...|I feel like this ...|               21|    true|null|06 19, 2017|    1497830400| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A2JMHG...|B001IKJOLW|A2JMHGCCBZPYP4|            5.0|          Five Stars|Great Gift - she ...|               11|    true|null|12 29, 2016|    1482969600| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B0014F7B98-A2N97F...|B0014F7B98|A2N97FVE233YZ4|            5.0|          Five Stars|Really great for ...|               15|    true|null| 05 5, 2018|    1525478400| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A2P87P...|B001IKJOLW|A2P87PVYP2KLT6|            4.0|A little annoying...|I wear them for Z...|               30|    true|null|05 24, 2016|    1464048000| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A2ZMHB...|B001IKJOLW|A2ZMHBMNYFXOX7|            4.0|They fit as expec...|They fit as expec...|               27|    true|null|04 24, 2017|    1492992000| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-AT5OQF...|B001IKJOLW| AT5OQFDS6PEE1|            5.0|          Five Stars|The sneakers are ...|                9|    true|null|04 23, 2016|    1461369600| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B010RRWKT4-A135SG...|B010RRWKT4|A135SGOQMVWABQ|            5.0|          Five Stars|I wear these ever...|                9|    true|null|06 16, 2018|    1529107200| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A3JTZ6...|B001IKJOLW|A3JTZ664IGZIOK|            4.0|          Four Stars|        Nice product|                2|    true|null|06 19, 2017|    1497830400| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B010RRWKT4-A2077N...|B010RRWKT4|A2077NII5H62R2|            5.0|          Five Stars|        Perfect fit!|                2|    true|null| 07 2, 2018|    1530489600| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A2NKNE...|B001IKJOLW|A2NKNE9SV2KIN2|            5.0|  I love these shoes|I love these shoe...|               21|    true|null|09 26, 2016|    1474848000| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-AZYHPR...|B001IKJOLW| AZYHPRWLMSY9O|            5.0|          Five Stars|          Look great|                2|    true|null| 03 6, 2017|    1488758400| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B000YFSR5G-A36GNV...|B000YFSR5G|A36GNVEQP92OPA|            5.0|          Five Stars|Love 'em, just th...|               15|    true|null| 02 4, 2015|    1423008000| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B0017LD0BM-A36GNV...|B0017LD0BM|A36GNVEQP92OPA|            2.0|                 Huh|I'm not sure I wa...|               26|    true|null|05 21, 2015|    1432166400| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-ASOSP4...|B001IKJOLW| ASOSP4VAUDB7I|            4.0|I liked these sho...|I liked these sho...|               36|    true|null| 09 2, 2016|    1472774400| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A2AGT9...|B001IKJOLW|A2AGT9INDKDK6B|            5.0|Look amazing, run...|Love them!  I ord...|               34|    true|null| 04 8, 2017|    1491609600| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-ASEXDK...|B001IKJOLW| ASEXDKPTYY14D|            5.0|I had the older g...|I had the older g...|               42|    true|null|12 23, 2016|    1482451200| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-AVZB7U...|B001IKJOLW|  AVZB7UFLT6VD|            5.0|          Five Stars|Great shoe, very ...|                4|    true|null|09 13, 2017|    1505260800| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A1EF7Y...|B001IKJOLW|A1EF7YWCNN7E9K|            5.0|     Nike runs small|Ordered half a si...|               13|    true|null|09 27, 2017|    1506470400| null|http://deepyeti.u...|\n",
      "|AMAZON_FASHION_5|B001IKJOLW-A8I41E...|B001IKJOLW| A8I41EKH4UI1Y|            5.0|        Nice Support|Great arch suppor...|               17|    true|null|01 22, 2017|    1485043200| null|http://deepyeti.u...|\n",
      "+----------------+--------------------+----------+--------------+---------------+--------------------+--------------------+-----------------+--------+----+-----------+--------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a487051e-ac42-4309-83f0-b868b2106d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"file://\"+SparkFiles.get(file),schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce71b7-5e6c-45b7-ace5-161c70add331",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    "    StructField(\"overall\",IntegerType(),True), \n",
    "    StructField(\"vote\",IntegerType(),True), \n",
    "    StructField(\"verified\",BooleanType(),True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7c46f-7ccb-44f1-9aea-aeab41967b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e740718d-ffa8-4f44-a857-0524de156316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
