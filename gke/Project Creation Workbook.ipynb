{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compact-currency",
   "metadata": {},
   "source": [
    "# Google Cloud Platform Project Creation Workbook \n",
    " \n",
    "Use this workbook to create a google cloud project with everything needed to collect new data and host your own web app. \n",
    " \n",
    "Prerequisites:  \n",
    "+ Create Google user account  <br><br>\n",
    "+ Create your own personal Google Cloud Project and Enable Billing\n",
    "    - Enable Free Tier account by seleting \"Try it Free\" here: [Try Google Cloud Platform for free](https://cloud.google.com/cloud-console)\n",
    "    - Follow steps to activate billing found here: [Create New Billing Account](https://cloud.google.com/billing/docs/how-to/manage-billing-account#create_a_new_billing_account)\n",
    "        - Billing account is required for APIs used in this project\n",
    "        - You will not exceed the $300 free trial setting up this project but make sure to delete the project if you do not want to be charged\n",
    "        - Take note of project name created because this billing account will be used with the new project <br><br>\n",
    "+ Install and initialize Google Cloud SDK by following instructions found here: [Cloud SDK Quickstart](https://cloud.google.com/sdk/docs/quickstart) <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-kansas",
   "metadata": {},
   "source": [
    "## Step 1 - Check Prequisites Successfully Completed\n",
    "Check that you have successfully installed and enabled Cloud SDK by running the config list command. If you get an error please refer to Troubleshooting steps found here [Cloud SDK Quickstart](https://cloud.google.com/sdk/docs/quickstart).  \n",
    "You should see an output that includes your account along with any other configuration setup when using gcloud init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "junior-insurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[accessibility]\n",
      "screen_reader = False\n",
      "[compute]\n",
      "region = us-central1\n",
      "zone = us-central1-c\n",
      "[core]\n",
      "account = cwilbar@alumni.nd.edu\n",
      "disable_usage_reporting = False\n",
      "project = spark-container-testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Your active configuration is: [default]\n"
     ]
    }
   ],
   "source": [
    "!gcloud config list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a37d5e7-4bbf-4c46-8ca7-027a1ab2008a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=h2qCORTEgmAkwH9QtYs2vrMNPTy77M&access_type=offline&code_challenge=KGWWdOdNXOO2Te0W4SZEOwTMxJxpOU3-V04z9QDABpc&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [cwilbar@alumni.nd.edu].\n",
      "Your current project is [spark-container-testing].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n"
     ]
    }
   ],
   "source": [
    "#!gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-cooperation",
   "metadata": {},
   "source": [
    "## Step 2 - Create GCP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prescription-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### TO DO: Enter name for new project\n",
    "###### Note: Proect name must be unique across GCP. If you get error when creating project please change the project name here and try again.\n",
    "\n",
    "new_project_id = 'spark-container-testing-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "local-hollow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Create in progress for [https://cloudresourcemanager.googleapis.com/v1/projects/spark-container-testing-2].\n",
      "Waiting for [operations/cp.7465891856551616272] to finish...\n",
      "..done.\n",
      "Enabling service [cloudapis.googleapis.com] on project [spark-container-testing-2]...\n",
      "Operation \"operations/acf.p2-448485601169-a932ca8e-b263-4369-89c2-02afc6c216da\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects create {new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76a11c3-888c-4fe4-b65a-1071443c6c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-teacher",
   "metadata": {},
   "source": [
    "#### IMPORTANT\n",
    "*****TO DO: Navigate to [Cloud Console](https://console.cloud.google.com/), Change to new project, and enable billing following instructions found here: [Enable Billing](https://cloud.google.com/billing/docs/how-to/modify-project#enable_billing_for_a_project)***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-prediction",
   "metadata": {},
   "source": [
    "## Step 3 - Enable Necessary Cloud Services\n",
    "\n",
    "This project uses:\n",
    "+ Google Kubernetes Engine for a kubernetes cluster manager\n",
    "+ Google Container Registry to store spark Docker container images\n",
    "  \n",
    "List below contains all services needed at time of creation of this workbook. Please add/remove from this list if the names/necessary services have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hourly-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_services_list = [\n",
    "    'bigquery.googleapis.com',\n",
    "    'bigquerystorage.googleapis.com',\n",
    "    'cloudapis.googleapis.com',\n",
    "    'cloudbuild.googleapis.com',\n",
    "    'clouddebugger.googleapis.com',\n",
    "    'cloudtrace.googleapis.com',\n",
    "    'compute.googleapis.com',\n",
    "    'container.googleapis.com',\n",
    "    'containeranalysis.googleapis.com',\n",
    "    'containerregistry.googleapis.com',\n",
    "    'iam.googleapis.com ',\n",
    "    'iamcredentials.googleapis.com ',\n",
    "    'language.googleapis.com',\n",
    "    'oslogin.googleapis.com',\n",
    "    'servicemanagement.googleapis.com',\n",
    "    'serviceusage.googleapis.com',\n",
    "    'sql-component.googleapis.com',\n",
    "    'storage-api.googleapis.com',\n",
    "    'storage-component.googleapis.com',\n",
    "    'storage.googleapis.com'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "touched-poverty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Operation \"operations/acf.p2-448485601169-cc5b0618-1ffb-41b9-b130-a52f58d738ef\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "## Services can only be enabled 20 at a time at the time of workbook creation. Use this loop to enable 20 at a time.\n",
    "for x in range(0,len(enable_services_list),20):\n",
    "    !gcloud services enable {' '.join(enable_services_list[x:(x+20)])} --project={new_project_id}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "suspended-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                              TITLE\n",
      "bigquery.googleapis.com           BigQuery API\n",
      "bigquerystorage.googleapis.com    BigQuery Storage API\n",
      "cloudapis.googleapis.com          Google Cloud APIs\n",
      "cloudbuild.googleapis.com         Cloud Build API\n",
      "clouddebugger.googleapis.com      Cloud Debugger API\n",
      "cloudtrace.googleapis.com         Cloud Trace API\n",
      "compute.googleapis.com            Compute Engine API\n",
      "container.googleapis.com          Kubernetes Engine API\n",
      "containeranalysis.googleapis.com  Container Analysis API\n",
      "containerregistry.googleapis.com  Container Registry API\n",
      "datastore.googleapis.com          Cloud Datastore API\n",
      "iam.googleapis.com                Identity and Access Management (IAM) API\n",
      "iamcredentials.googleapis.com     IAM Service Account Credentials API\n",
      "language.googleapis.com           Cloud Natural Language API\n",
      "logging.googleapis.com            Cloud Logging API\n",
      "monitoring.googleapis.com         Cloud Monitoring API\n",
      "oslogin.googleapis.com            Cloud OS Login API\n",
      "pubsub.googleapis.com             Cloud Pub/Sub API\n",
      "servicemanagement.googleapis.com  Service Management API\n",
      "serviceusage.googleapis.com       Service Usage API\n",
      "sql-component.googleapis.com      Cloud SQL\n",
      "storage-api.googleapis.com        Google Cloud Storage JSON API\n",
      "storage-component.googleapis.com  Cloud Storage\n",
      "storage.googleapis.com            Cloud Storage API\n"
     ]
    }
   ],
   "source": [
    "# Check that services were enabled\n",
    "!gcloud services list --project={new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-recipient",
   "metadata": {},
   "source": [
    "## Step 4 - Create Necessary Service Accounts\n",
    "\n",
    "There are two primary service accounts used in this project:  \n",
    "- **Deployment Service Account**\n",
    "    - We create this and add necessary roles below using the Cloud SDK\n",
    "    - deployer-sa@your_project_name.iam.gserviceaccount.com\n",
    "    - This account is used to deploy and test docker container and kubernetes cluster<br><br>\n",
    "- **BigQuery Service Account**\n",
    "    - We create this and add necessary roles below using the Cloud SDK\n",
    "    - bigquery-sa@your_project_name.iam.gserviceaccount.com\n",
    "    - This account is used in the container for access to big query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-climate",
   "metadata": {},
   "source": [
    "Check what service ccounts are already created (should be the two default ones described above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "joint-trinity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPLAY NAME                            EMAIL                                               DISABLED\n",
      "Compute Engine default service account  448485601169-compute@developer.gserviceaccount.com  False\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts list --project={new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "marked-costs",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created service account [deployer-sa].\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts create deployer-sa \\\n",
    "    --display-name=\"Deployment Service Account\" \\\n",
    "    --description=\"Account used to deploy to Google Cloud Project\" \\\n",
    "    --project={new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "graphic-juice",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created service account [bigquery-sa].\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts create bigquery-sa \\\n",
    "    --display-name=\"BigQuery Service Account\" \\\n",
    "    --description=\"Account used by Spark Containers to Connect to BigQuery\" \\\n",
    "    --project={new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-junior",
   "metadata": {},
   "source": [
    "Check service accounts were created successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "attended-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPLAY NAME                            EMAIL                                                          DISABLED\n",
      "Deployment Service Account              deployer-sa@spark-container-testing-2.iam.gserviceaccount.com  False\n",
      "BigQuery Service Account                bigquery-sa@spark-container-testing-2.iam.gserviceaccount.com  False\n",
      "Compute Engine default service account  448485601169-compute@developer.gserviceaccount.com             False\n"
     ]
    }
   ],
   "source": [
    "!gcloud iam service-accounts list --project={new_project_id}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-collect",
   "metadata": {},
   "source": [
    "Programatically update the roles for the new service accounts using the guide found here: [Programatic Change Access](https://cloud.google.com/iam/docs/granting-changing-revoking-access#programmatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "useful-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save policy file in directory above where the repo is saved so that it is not stored to github\n",
    "file_directory = '..\\..\\policy.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "golden-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write current policy to file directory\n",
    "!gcloud projects get-iam-policy {new_project_id} --format json > {file_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-bundle",
   "metadata": {},
   "source": [
    "**If running jupyter notebook run below cell to load and modify policy file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "express-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('..\\..\\policy.json') as f:\n",
    "    policy = json.load(f)\n",
    "\n",
    "def modify_policy_add_role(policy, role, member):\n",
    "    \"\"\"Adds a new role binding to a policy.\"\"\"\n",
    "\n",
    "    binding = {\"members\": [member],\"role\": role }\n",
    "    policy[\"bindings\"].append(binding)\n",
    "    return policy\n",
    "\n",
    "members = [f'serviceAccount:deployer-sa@{new_project_id}.iam.gserviceaccount.com', \n",
    "           f'serviceAccount:bigquery-sa@{new_project_id}.iam.gserviceaccount.com']\n",
    "roles = {\n",
    "        members[0]:['roles/editor'],\n",
    "        members[1]:['roles/bigquery.dataEditor','roles/run.serviceAgent', 'roles/bigquery.user',\n",
    "                    'roles/storage.admin']}\n",
    "\n",
    "for member in members:\n",
    "    for role in roles[member]:\n",
    "        policy = modify_policy_add_role(policy, role, member)\n",
    "\n",
    "with open('..\\..\\policy.json', 'w') as json_file:\n",
    "    json.dump(policy, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "opening-command",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bindings:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated IAM policy for project [spark-container-testing-2].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-container-testing-2.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.dataEditor\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-container-testing-2.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.user\n",
      "- members:\n",
      "  - serviceAccount:448485601169@cloudbuild.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.builder\n",
      "- members:\n",
      "  - serviceAccount:service-448485601169@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-448485601169@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-448485601169@container-engine-robot.iam.gserviceaccount.com\n",
      "  role: roles/container.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-448485601169@container-analysis.iam.gserviceaccount.com\n",
      "  role: roles/containeranalysis.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-448485601169@containerregistry.iam.gserviceaccount.com\n",
      "  role: roles/containerregistry.ServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:448485601169-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:448485601169@cloudservices.gserviceaccount.com\n",
      "  - serviceAccount:deployer-sa@spark-container-testing-2.iam.gserviceaccount.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - user:cwilbar@alumni.nd.edu\n",
      "  role: roles/owner\n",
      "- members:\n",
      "  - serviceAccount:service-448485601169@gcp-sa-pubsub.iam.gserviceaccount.com\n",
      "  role: roles/pubsub.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-container-testing-2.iam.gserviceaccount.com\n",
      "  role: roles/run.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:bigquery-sa@spark-container-testing-2.iam.gserviceaccount.com\n",
      "  role: roles/storage.admin\n",
      "etag: BwXDpJSwq6g=\n",
      "version: 1\n"
     ]
    }
   ],
   "source": [
    "!gcloud projects set-iam-policy {new_project_id} {file_directory}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "governmental-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove policy file \n",
    "!del {file_directory}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-hierarchy",
   "metadata": {},
   "source": [
    "## Step 5 - Create Kubernetes Engine Cluster\n",
    "\n",
    "In order to deploy a container to kubernetes to run an application you first need to create a kubernetes engine cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "wanted-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: Change region  to your default region\n",
    "COMPUTE_REGION = 'us-central1'\n",
    "CLUSTER_NAME = 'spark-cluster'\n",
    "COMPUTE_ZONE = 'us-central1-c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9a0ea5c-e21f-44f7-8a4e-c860addfda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud compute regions list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54668c69-81c1-4f6f-9f2b-32a1876f7449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set compute/region {COMPUTE_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d28174a-6abb-4b05-94b1-2d74673d14b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [compute/zone].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set compute/zone {COMPUTE_ZONE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36a14f55-1192-40f4-a028-f642d2705973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           LOCATION       MASTER_VERSION   MASTER_IP       MACHINE_TYPE  NODE_VERSION     NUM_NODES  STATUS\n",
      "spark-cluster  us-central1-c  1.19.9-gke.1400  104.198.69.202  e2-medium     1.19.9-gke.1400  3          RUNNING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Starting in January 2021, clusters will use the Regular release channel by default when `--cluster-version`, `--release-channel`, `--no-enable-autoupgrade`, and `--no-enable-autorepair` flags are not specified.\n",
      "WARNING: Currently VPC-native is not the default mode during cluster creation. In the future, this will become the default mode and can be disabled using `--no-enable-ip-alias` flag. Use `--[no-]enable-ip-alias` flag to suppress this warning.\n",
      "WARNING: Starting with version 1.18, clusters will have shielded GKE nodes by default.\n",
      "WARNING: Your Pod address range (`--cluster-ipv4-cidr`) can accommodate at most 1008 node(s). \n",
      "WARNING: Starting with version 1.19, newly created clusters and node-pools will have COS_CONTAINERD as the default node image when no image type is specified.\n",
      "Creating cluster spark-cluster in us-central1-c...\n",
      "..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................done.\n",
      "Created [https://container.googleapis.com/v1/projects/spark-container-testing-2/zones/us-central1-c/clusters/spark-cluster].\n",
      "To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-central1-c/spark-cluster?project=spark-container-testing-2\n",
      "kubeconfig entry generated for spark-cluster.\n"
     ]
    }
   ],
   "source": [
    "# Create cluster using auto-pilot mode. This may take serveral minutes\n",
    "!gcloud container clusters create {CLUSTER_NAME} \\\n",
    "    --project={new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e2def4f-291f-40d1-b0f1-e3603cf5ee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching cluster endpoint and auth data.\n",
      "kubeconfig entry generated for spark-cluster.\n"
     ]
    }
   ],
   "source": [
    "# Get credentials to use when deploying to cluster\n",
    "!gcloud container clusters get-credentials {CLUSTER_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bbd6f08-dd35-45fa-8c90-0fb8bf2f2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "account = f'bigquery-sa@{new_project_id}.iam.gserviceaccount.com' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3cb2bb8-eea8-4795-a21a-c69bf6b320d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "created key [f00f27c27428f8e7d0ff4f477de6d8d3117c4f44] of type [json] as [sa.json] for [bigquery-sa@spark-container-testing-2.iam.gserviceaccount.com]\n"
     ]
    }
   ],
   "source": [
    "# Download bigquery service account json file\n",
    "!gcloud iam service-accounts keys create sa.json \\\n",
    "    --iam-account={account}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dc2fc8d-2315-4326-bbb1-702db5e20fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secret/bigquery-credentials created\n"
     ]
    }
   ],
   "source": [
    "# Create Kubernetes Secret from file\n",
    "!kubectl create secret generic bigquery-credentials \\\n",
    "  --from-file ./sa.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-inspection",
   "metadata": {},
   "source": [
    "## Step 6 - Create BigQuery Dataset\n",
    "\n",
    "Your new project will need a dataset to store the data if you plan on copying/creating your own repository of data.  \n",
    "\n",
    "This has to be a unique name per project.  \n",
    "\n",
    "In my workflows I have named the dataset 'nba' but feel free to change it. Note that if you do change it, then you will also need to change the dataset name in any of the other python scripts in this project appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "empirical-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'amazon_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "marked-montana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'spark-container-testing-2:amazon_reviews' successfully created.\n"
     ]
    }
   ],
   "source": [
    "#Stop and re-run if this takes more than a minute\n",
    "!bq --location=US mk --dataset \\\n",
    "--description \"Stores transformed amazon review data orginally found at https://nijianmo.github.io/amazon/index.html\" \\\n",
    "{new_project_id}:{dataset_name}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-missouri",
   "metadata": {},
   "source": [
    "## Step 7 - Build and Push Container to GCR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "congressional-sullivan",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 sha256:f114d320a0272f19261b14dccc1e29a48d372e6202490fa5ecacb843176f1b4b\n",
      "#1 transferring dockerfile: 3.92kB done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:5d04353d56298a3d137e7193b4accc66a64c2551e1032ecd5b771de87cafe346\n",
      "#2 transferring context: 35B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#4 [ 1/16] FROM docker.io/library/ubuntu:latest\n",
      "#4 sha256:0a5f349eacf4edfd2fc1577c637ef52a2ed3280d9d5c0ab7f2e4c4052e7d6c9f\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#11 https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.0.jar\n",
      "#11 sha256:173f6ef3aef50bc006d358109ed87a9ceb38ec7b8570fecb583413853f5055c7\n",
      "#11 DONE 0.2s\n",
      "\n",
      "#14 https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.20.0.jar\n",
      "#14 sha256:89b5ffef7e782b56dfd9b6af19658592f873337a5886ddeb5a48695eb67d8fbf\n",
      "#14 DONE 0.2s\n",
      "\n",
      "#8 [ 5/16] RUN echo \"alias pyspark=/opt/spark/bin/pyspark\" >> ~/.bashrc &&     echo \"alias spark-shell=/opt/spark/bin/spark-shell\" >> ~/.bashrc\n",
      "#8 sha256:3ea53355bf7abd8ab5ceb5f4668794a2eebe395b7b00e08ada61fbad7aa5eaf9\n",
      "#8 CACHED\n",
      "\n",
      "#5 [ 2/16] RUN apt-get -y update &&     apt-get install --no-install-recommends -y     curl     tini     wget     python     python3-pip     \"openjdk-14-jre-headless\"     ca-certificates-java &&     apt-get clean && rm -rf /var/lib/apt/lists/*\n",
      "#5 sha256:05b07e83510f11d2d15850c8a67f93a7445066e48f649f744fffae2c288655f7\n",
      "#5 CACHED\n",
      "\n",
      "#7 [ 4/16] RUN wget -q $(wget -qO- https://www.apache.org/dyn/closer.lua/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz?as_json |     python3 -c \"import sys, json; content=json.load(sys.stdin); print(content['preferred']+content['path_info'])\") &&     echo \"E90B31E58F6D95A42900BA4D288261D71F6C19FA39C1CB71862B792D1B5564941A320227F6AB0E09D946F16B8C1969ED2DEA2A369EC8F9D2D7099189234DE1BE *spark-3.1.1-bin-hadoop3.2.tgz\" | sha512sum -c - &&     tar xzf \"spark-3.1.1-bin-hadoop3.2.tgz\" -C /opt --owner root --group root --no-same-owner &&     mv /opt/spark-3.1.1-bin-hadoop3.2 /opt/spark &&     rm \"spark-3.1.1-bin-hadoop3.2.tgz\"\n",
      "#7 sha256:32558d1ed6828552970cffd170e99f07f8a9a0d8a1a715baf62575f4272c0d8f\n",
      "#7 CACHED\n",
      "\n",
      "#9 [ 6/16] RUN useradd -l -m -s /bin/bash -N -u 1000 joyvan &&     chmod g+w /etc/passwd\n",
      "#9 sha256:e7c0865262892a84d1989fa0601c4ba3c131445c25c97095dce2bc2d51a16da4\n",
      "#9 CACHED\n",
      "\n",
      "#12 [ 8/16] ADD https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.0.jar /opt/spark/jars\n",
      "#12 sha256:f4d4c6ae05dea13a2f277758f25faf923c028528c2fc7af7cfe1ddf0c9c36361\n",
      "#12 CACHED\n",
      "\n",
      "#10 [ 7/16] WORKDIR /opt/spark/confs\n",
      "#10 sha256:d4dc1b3ec3f5cee1e63d90b9d66fdeabc4413b09d6deb48f12ffb262a3fb2890\n",
      "#10 CACHED\n",
      "\n",
      "#15 [10/16] ADD https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.20.0.jar /opt/spark/jars\n",
      "#15 sha256:7efbb66b7215096320278949108e8a0285b4b741cb7d8aed6f022d3b1ce25ec9\n",
      "#15 CACHED\n",
      "\n",
      "#6 [ 3/16] WORKDIR /tmp\n",
      "#6 sha256:914763bcb80e0169f10079f2122ddbdf4dfa941707b769d4b860c054f9ea5d06\n",
      "#6 CACHED\n",
      "\n",
      "#13 [ 9/16] RUN chmod 644 /opt/spark/jars/gcs-connector-hadoop3-2.2.0.jar\n",
      "#13 sha256:30f34966fdb0073492dc7483385daea11896498e82a420fd56f31cca6a57dc07\n",
      "#13 CACHED\n",
      "\n",
      "#16 [11/16] RUN chmod 644 /opt/spark/jars/spark-bigquery-with-dependencies_2.12-0.20.0.jar\n",
      "#16 sha256:e1efa05d2bc49fcf8637ee2b172d30f28c764b0e186021d8698134947d9fba3e\n",
      "#16 CACHED\n",
      "\n",
      "#17 [12/16] RUN cp /opt/spark/conf/spark-defaults.conf.template /opt/spark/conf/spark-defaults.conf &&     echo spark.hadoop.google.cloud.auth.service.account.enable       true >> spark-defaults.conf &&     echo spark.hadoop.google.cloud.auth.service.account.json.keyfile        /var/secrets/google/sa.json>> spark-defaults.conf &&     echo spark.hadoop.fs.gs.AbstractFilePath.impl     com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS >> spark-defaults.conf\n",
      "#17 sha256:c6b2fdcad1eea6938e9d53ca34aecc3e75de6c969167c6ecf692f298cdd6197a\n",
      "#17 DONE 1.3s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/library/ubuntu:latest\n",
      "#3 sha256:8c6bdfb121a69744f11ffa1fedfc68ec20085c2dcce567aac97a3ff72e53502d\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#18 [internal] load build context\n",
      "#18 sha256:152de94fad3de9986b8ac00a30f6c7beeb7cbf48eb0ef5d7a87ac73247578151\n",
      "#18 transferring context: 238B done\n",
      "#18 DONE 0.0s\n",
      "\n",
      "#23 exporting to image\n",
      "#23 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n",
      "#23 exporting layers\n",
      "#23 exporting layers 6.8s done\n",
      "#23 writing image sha256:7c1be9d36bfb3bdff8a66d27ef68ed20b42fc20850266448871fae742942b268 done\n",
      "#23 naming to docker.io/library/client-mode-spark-notebook done\n",
      "#23 DONE 7.0s\n"
     ]
    }
   ],
   "source": [
    "!docker build -t client-mode-spark-notebook ../client-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a8b079a-c3a4-48eb-bd18-7ba651f6174a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker tag client-mode-spark-notebook:latest gcr.io/{new_project_id}/client-mode-spark-notebook:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38606448-d0e9-4b90-91a1-48e190ca570f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker push gcr.io/{new_project_id}/client-mode-spark-notebook:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-sheep",
   "metadata": {},
   "source": [
    "## Step 8 - Deploy App to Cluster and Expose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39f73d4f-a3b8-4cb0-882c-8329d3b624d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = 'spark-server'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b601920-db48-4ad0-8a93-a437651ff7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/spark-server created\n"
     ]
    }
   ],
   "source": [
    "!kubectl create deployment {APP_NAME} --image=gcr.io/{new_project_id}/client-mode-spark-notebook:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e52d1c22-49fa-4e9a-9003-23186eac006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: error validating \"deployment.yaml\": error validating data: [ValidationError(Deployment.spec.template.spec): unknown field \"volumeMounts\" in io.k8s.api.core.v1.PodSpec, ValidationError(Deployment.spec): unknown field \"volumes\" in io.k8s.api.apps.v1.DeploymentSpec]; if you choose to ignore these errors, turn validation off with --validate=false\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f deployment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06769385-3d31-42f7-a396-665de5883c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/spark-server exposed\n"
     ]
    }
   ],
   "source": [
    "!kubectl expose deployment {APP_NAME} --type LoadBalancer --port 80 --target-port 8888"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-chest",
   "metadata": {},
   "source": [
    "## Step 9 - Create BigQuery View\n",
    "\n",
    "In order to use the nba_model_game_refresh function we need to create a Big Query view that identifies what games have been loaded in to the raw_basektballrefernce_game table but have not been loaded in to the model_game_data table yet. Copying datasets does not copy views so we will always need to run this step even if you copied the entire dataset directly.\n",
    "\n",
    "**IMPORTANT** If you ever change the number of games to use for the weighted moving average (W) then you will need to update this view as well. The game_number < filter needs to change to however many games you are averaging over. Future release will seek to remove this change dependency as it is too easy to miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change dataset name (nba) if you chose a different dataset name earlier\n",
    "view_name = 'nba.games_to_load_to_model'\n",
    "view_query = f'CREATE OR REPLACE VIEW `{view_name}` AS \\\n",
    "WITH model_load_games as (SELECT \\\n",
    "distinct game_key as game_key \\\n",
    "FROM `nba.model_game` \\\n",
    ") \\\n",
    "    SELECT distinct order_of_games_per_team.game_key, \\\n",
    "    CASE WHEN model_load_games.game_key is NULL THEN 1 ELSE 0 END as NEEDS_TO_LOAD_TO_MODEL \\\n",
    "    FROM ( \\\n",
    "            SELECT team, game_key, row_number() OVER (PARTITION BY team ORDER BY game_date desc) as game_number \\\n",
    "            FROM ( \\\n",
    "                    SELECT \\\n",
    "                        home_team_name as team, game_date, game_key \\\n",
    "                    FROM  `nba.raw_basketballreference_game` \\\n",
    "                    UNION DISTINCT \\\n",
    "                    SELECT \\\n",
    "                        visitor_team_name as team, game_date, game_key \\\n",
    "                    FROM  `nba.raw_basketballreference_game` \\\n",
    "                 ) games_per_team \\\n",
    "            )order_of_games_per_team \\\n",
    "    LEFT JOIN model_load_games ON model_load_games.game_key = order_of_games_per_team.game_key \\\n",
    "    WHERE team in ( \\\n",
    "                    SELECT \\\n",
    "                        distinct home_team_name as team_to_load \\\n",
    "                    FROM `nba.raw_basketballreference_game` \\\n",
    "                    WHERE \\\n",
    "                        game_key not in (SELECT game_key FROM model_load_games) \\\n",
    "                    UNION DISTINCT \\\n",
    "                    SELECT \\\n",
    "                        distinct visitor_team_name as team_to_load \\\n",
    "                    FROM `nba.raw_basketballreference_game` \\\n",
    "                    WHERE \\\n",
    "                        game_key not in (SELECT game_key FROM model_load_games))'\n",
    "\n",
    "run_view = f'''bq query --use_legacy_sql=false --project_id={new_project_id} \"{view_query}\"'''\n",
    "!{run_view}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-championship",
   "metadata": {},
   "source": [
    "## Step 10 - Create Cloud Scheduler Jobs\n",
    "\n",
    "This is only required if you wish to keep your data up to date. If you do not need to keep the data up to date, simply make sure you execute the nba_model_game_refresh and nba_get_upcoming_games functions once in order for the Web App to be able to function with most recent game and upcoming schedule information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: Replace region with the region your cloud functions are deployed to and timezone with your desired scheduled time zone\n",
    "region = 'us-central1'\n",
    "timezone = 'America/Chicago'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create daily scraper schedule\n",
    "uri = f'https://{region}-{new_project_id}.cloudfunctions.net/nba_basketball_reference_scraper'\n",
    "!gcloud scheduler jobs create http nba_basketball_reference_scraper_daily --project {new_project_id} \\\n",
    "--schedule \"0 6 * * *\" --uri {uri} --http-method GET \\\n",
    "--time-zone={timezone} \\\n",
    "--description=\"Calls http cloud function nba_basketball_reference_scraper every day to scrape the most recent days information and add to big query tables\"\n",
    "\n",
    "# Create daily model refresh schedule\n",
    "uri = f'https://{region}-{new_project_id}.cloudfunctions.net/nba_model_game_refresh'\n",
    "!gcloud scheduler jobs create http nba_model_game_refresh_daily --project {new_project_id} \\\n",
    "--schedule \"0 7 * * *\" --uri {uri} --http-method GET \\\n",
    "--time-zone={timezone} \\\n",
    "--description=\"Calls http cloud function nba_model_game_refresh every day to load the most recently scraped data in to the model table and most recent data for each team to firestore\"\n",
    "\n",
    "# Create upcoming games refresh schedule\n",
    "uri = f'https://{region}-{new_project_id}.cloudfunctions.net/nba_get_upcoming_games'\n",
    "!gcloud scheduler jobs create http nba_get_upcoming_games --project {new_project_id} \\\n",
    "--schedule \"0 5 * * *\" --uri {uri} --http-method GET \\\n",
    "--time-zone={timezone} \\\n",
    "--description=\"Calls http cloud function nba_get_upcoming_games every day to scrape the schedule for the upcoming week and store to cloud storage\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-porter",
   "metadata": {},
   "source": [
    "## Step 11 - Trigger Cloud Functions\n",
    "\n",
    "In order to populate Firestore with the most recent game data and cloud storage with the upcoming games the fdeploy functions must be triggered. This can be done in the Console or by using the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_data = '{}'\n",
    "empty_data = json.dumps(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud functions call --project {new_project_id} nba_basketball_reference_scraper --data {empty_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud functions call nba_model_game_refresh --project {new_project_id} --data {empty_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud functions call nba_get_upcoming_games --project {new_project_id} --data {empty_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-reflection",
   "metadata": {},
   "source": [
    "## Step 12 - Create Static Model Training Data View\n",
    "\n",
    "For tranparency and auditability we create a view using the model_game table for specific dates and a timestamped name. This will allow us to come back to train different models on the same data. These are created as views so they are not part of what is copied externally but you could create these as tables instead if desired but would have to pay for additional storage costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Change timezone to your timezone if desired\n",
    "timezone = 'America/Chicago'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create view that excludes first game in every seasons because rest days will be way off. \n",
    "#It will use moving average dating back to previous season.\n",
    "\n",
    "query = f\"\"\"EXECUTE IMMEDIATE CONCAT(' \\\n",
    "                CREATE OR REPLACE VIEW `nba.model_training_data_' \\\n",
    "                , FORMAT_DATE('%Y%m%d', CURRENT_DATE(\\\\\"{timezone}\\\\\")) \\\n",
    "            ,'` AS \\\n",
    "                SELECT * FROM ( \\\n",
    "                    SELECT \\\n",
    "                        *, \\\n",
    "                        ROW_NUMBER() OVER (PARTITION BY g.SEASON, g.TEAM ORDER BY g.game_date asc) as SEASON_GAME_NUMBER, \\\n",
    "                    FROM nba.model_game g \\\n",
    "            ) WHERE SEASON_GAME_NUMBER > 1 and is_home_team = 1 \\\n",
    "                and game_date < DATE_SUB(CURRENT_DATE(\\\\\"{timezone}\\\\\"), INTERVAL 1 WEEK)')\"\"\"\n",
    "\n",
    "run_query = f'''bq query --use_legacy_sql=false --project_id={new_project_id} \"{query}\"'''\n",
    "\n",
    "!{run_query}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-principal",
   "metadata": {},
   "source": [
    "## Step 13 - Create Baseline Linear Model using View\n",
    "\n",
    "We will now use the data in the view we just created to generate a linear model on all of the relevant variables. \n",
    "\n",
    "You definiltey will want to open the Console to explore the model further but that is left as a separate task.\n",
    "\n",
    "**NOTE:** This is the most time consuming and costly step. Be careful with running this too many times but definitely expirement with different modeling types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If running Step 13 on the same date as Step 12 execute this cell to set the view date\n",
    "from datetime import datetime\n",
    "view_date = datetime.now().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-pathology",
   "metadata": {},
   "source": [
    "##If running Step 13 on a different day than Step 12 change the date here to the date you created the view in Step 12  \n",
    "view_date = '20210310'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_query = f\"\"\"CREATE OR REPLACE MODEL nba.baseline_linear_model \\\n",
    "  OPTIONS(model_type='LINEAR_REG', input_label_cols=['spread']) \\\n",
    "    AS SELECT spread, \\\n",
    "        is_home_team, \\\n",
    "        incoming_is_win_streak, \\\n",
    "        incoming_is_win_streak_opponent, \\\n",
    "        incoming_wma_{W}_pace, \\\n",
    "        incoming_wma_{W}_efg_pct, \\\n",
    "        incoming_wma_{W}_tov_pct, \\\n",
    "        incoming_wma_{W}_ft_rate, \\\n",
    "        incoming_wma_{W}_off_rtg, \\\n",
    "        incoming_wma_{W}_opponent_efg_pct, \\\n",
    "        incoming_wma_{W}_opponent_tov_pct, \\\n",
    "        incoming_wma_{W}_opponent_ft_rate, \\\n",
    "        incoming_wma_{W}_opponent_off_rtg, \\\n",
    "        incoming_wma_{W}_starter_minutes_played_proportion, \\\n",
    "        incoming_wma_{W}_bench_plus_minus,\\\n",
    "        incoming_wma_{W}_opponnent_starter_minutes_played_proportion, \\\n",
    "        incoming_wma_{W}_opponent_bench_plus_minus, \\\n",
    "        incoming_rest_days - incoming_rest_days_opponent as rest_days_difference \\\n",
    "    FROM `nba.model_training_data_{view_date}`\"\"\"\n",
    "\n",
    "model_query = f'''bq query --use_legacy_sql=false --project_id={new_project_id} \"{model_query}\"'''\n",
    "\n",
    "!{model_query}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-square",
   "metadata": {},
   "source": [
    "## Step 14 Deploy App Engine App\n",
    "\n",
    "We are finally ready to deploy the app engine web app! If you have sucessfully completed all steps above then you should be able to navigate to a webpage that works the same as the [webpage](https://nba-predictions-prod.uc.r.appspot.com/) in the Readme.\n",
    "\n",
    "As a prequisite, make sure you are running this notebook in the folder from the gitclone or be sure to replace the file paths below with the correct file path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud app deploy ../webapp/app.yaml --project={new_project_id} --promote --quiet\n",
    "print(f'Check you your new web page at https://{new_project_id}.uc.r.appspot.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-aberdeen",
   "metadata": {},
   "source": [
    "## Optional - Delete Project\n",
    "\n",
    "To avoid on-going charges for everything created in this workbook run the below command to delete the project that you just created. Note it will take approximately 30 days for full completion and you will stil be charged for any charges accrued during this walkthrough. Check out [Deleting GCP Project](https://cloud.google.com/resource-manager/docs/creating-managing-projects?visit_id=637510410447506984-2569255859&rd=1#shutting_down_projects) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment code to delete project\n",
    "# !gcloud projects delete {new_project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "martial-southwest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
